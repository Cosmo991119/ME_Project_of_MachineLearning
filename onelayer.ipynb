{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkUvMHInHZB-",
        "outputId": "babbcd9e-0532-4c5c-b2df-01473d2c7bab"
      },
      "source": [
        "train data preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS8BaVe7ILTL"
      },
      "source": [
        "seed = 2020"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Efh2sSUIOO2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import time\n",
        "import math\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91OvtOtxIQfy"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H426jUXGIUgX"
      },
      "source": [
        "read data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "QTk_otrSITaA",
        "outputId": "b59aefcd-6064-4026-a319-20a253e28256"
      },
      "source": [
        "# 每一行数据如下\n",
        "# 'Hi.\\t嗨。\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #891077 (Martha)'\n",
        "with open('newdata', 'r', encoding='utf-8') as f:\n",
        "    data = f.read()\n",
        "data = data.strip()\n",
        "data = data.split('\\n')\n",
        "print('样本数:\\n', len(data))\n",
        "print('\\n样本示例:')\n",
        "data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "样本数:\n",
            " 10\n",
            "\n",
            "样本示例:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Two young, White males are outside near many bushes.\\tZwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Khafcn2DIm-E",
        "outputId": "385d7293-5d07-49e5-fac7-c6e46799b99c"
      },
      "source": [
        "en_data = [line.split('\\t')[0] for line in data]\n",
        "ch_data = [line.split('\\t')[1] for line in data]\n",
        "print('英文数据:\\n', en_data[:10])\n",
        "print('\\n德文数据:\\n', ch_data[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "英文数据:\n",
            " ['Two young, White males are outside near many bushes.', 'Several men in hard hats are operating a giant pulley system.', 'A little girl climbing into a wooden playhouse.', 'A man in a blue shirt is standing on a ladder cleaning a window.', 'Two men are at the stove preparing food.', 'A man in green holds a guitar while the other man observes his shirt.', 'A man is smiling at a stuffed lion', 'A trendy girl talking on her cellphone while gliding slowly down the street.', 'A woman with a large purse is walking by a gate.', 'Boys dancing on poles in the middle of the night.']\n",
            "\n",
            "德文数据:\n",
            " ['Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.', 'Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.', 'Ein kleines Mädchen klettert in ein Spielhaus aus Holz.', 'Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.', 'Zwei Männer stehen am Herd und bereiten Essen zu.', 'Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.', 'Ein Mann lächelt einen ausgestopften Löwen an.', 'Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.', 'Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei.', 'Jungen tanzen mitten in der Nacht auf Pfosten.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB29AYPIIn1C",
        "outputId": "287a74d3-fe57-412d-d124-7ab8e2d48ae4"
      },
      "source": [
        "# 按字符级切割，并添加<eos>\n",
        "en_token_list = [[char for char in line]+[\"<eos>\"] for line in en_data]\n",
        "ch_token_list = [[char for char in line]+[\"<eos>\"] for line in ch_data]\n",
        "print('英文数据:\\n', en_token_list[:2])\n",
        "print('\\n德文数据:\\n', ch_token_list[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "英文数据:\n",
            " [['T', 'w', 'o', ' ', 'y', 'o', 'u', 'n', 'g', ',', ' ', 'W', 'h', 'i', 't', 'e', ' ', 'm', 'a', 'l', 'e', 's', ' ', 'a', 'r', 'e', ' ', 'o', 'u', 't', 's', 'i', 'd', 'e', ' ', 'n', 'e', 'a', 'r', ' ', 'm', 'a', 'n', 'y', ' ', 'b', 'u', 's', 'h', 'e', 's', '.', '<eos>'], ['S', 'e', 'v', 'e', 'r', 'a', 'l', ' ', 'm', 'e', 'n', ' ', 'i', 'n', ' ', 'h', 'a', 'r', 'd', ' ', 'h', 'a', 't', 's', ' ', 'a', 'r', 'e', ' ', 'o', 'p', 'e', 'r', 'a', 't', 'i', 'n', 'g', ' ', 'a', ' ', 'g', 'i', 'a', 'n', 't', ' ', 'p', 'u', 'l', 'l', 'e', 'y', ' ', 's', 'y', 's', 't', 'e', 'm', '.', '<eos>']]\n",
            "\n",
            "德文数据:\n",
            " [['Z', 'w', 'e', 'i', ' ', 'j', 'u', 'n', 'g', 'e', ' ', 'w', 'e', 'i', 'ß', 'e', ' ', 'M', 'ä', 'n', 'n', 'e', 'r', ' ', 's', 'i', 'n', 'd', ' ', 'i', 'm', ' ', 'F', 'r', 'e', 'i', 'e', 'n', ' ', 'i', 'n', ' ', 'd', 'e', 'r', ' ', 'N', 'ä', 'h', 'e', ' ', 'v', 'i', 'e', 'l', 'e', 'r', ' ', 'B', 'ü', 's', 'c', 'h', 'e', '.', '<eos>'], ['M', 'e', 'h', 'r', 'e', 'r', 'e', ' ', 'M', 'ä', 'n', 'n', 'e', 'r', ' ', 'm', 'i', 't', ' ', 'S', 'c', 'h', 'u', 't', 'z', 'h', 'e', 'l', 'm', 'e', 'n', ' ', 'b', 'e', 'd', 'i', 'e', 'n', 'e', 'n', ' ', 'e', 'i', 'n', ' ', 'A', 'n', 't', 'r', 'i', 'e', 'b', 's', 'r', 'a', 'd', 's', 'y', 's', 't', 'e', 'm', '.', '<eos>']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NxLkoWkIrBi"
      },
      "source": [
        "# 基本字典\n",
        "basic_dict = {'<pad>':0, '<unk>':1, '<bos>':2, '<eos>':3}\n",
        "# 分别生成德英文字典 \n",
        "en_vocab = set(''.join(en_data))\n",
        "en2id = {char:i+len(basic_dict) for i, char in enumerate(en_vocab)}\n",
        "en2id.update(basic_dict)\n",
        "id2en = {v:k for k,v in en2id.items()}\n",
        "\n",
        "# 分别生成德英文字典 \n",
        "ch_vocab = set(''.join(ch_data))\n",
        "ch2id = {char:i+len(basic_dict) for i, char in enumerate(ch_vocab)}\n",
        "ch2id.update(basic_dict)\n",
        "id2ch = {v:k for k,v in ch2id.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6koaeID6ItKc",
        "outputId": "3b7d52a6-289f-4629-b8a2-2edc025dc499"
      },
      "source": [
        "# 利用字典，映射数据 \n",
        "en_num_data = [[en2id[en] for en in line ] for line in en_token_list]\n",
        "ch_num_data = [[ch2id[ch] for ch in line] for line in ch_token_list]\n",
        "\n",
        "print('char:', en_data[1])\n",
        "print('index:', en_num_data[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "char: Several men in hard hats are operating a giant pulley system.\n",
            "index: [14, 21, 17, 21, 29, 27, 11, 25, 12, 21, 16, 25, 4, 16, 25, 23, 27, 29, 22, 25, 23, 27, 10, 32, 25, 27, 29, 21, 25, 8, 7, 21, 29, 27, 10, 4, 16, 26, 25, 27, 25, 26, 4, 27, 16, 10, 25, 7, 24, 11, 11, 21, 6, 25, 32, 6, 32, 10, 21, 12, 5, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9X0Nnz7Iu8X"
      },
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_data, trg_data):\n",
        "        self.src_data = src_data\n",
        "        self.trg_data = trg_data\n",
        "\n",
        "        assert len(src_data) == len(trg_data), \\\n",
        "            \"numbers of src_data  and trg_data must be equal!\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_sample =self.src_data[idx]\n",
        "        src_len = len(self.src_data[idx])\n",
        "        trg_sample = self.trg_data[idx]\n",
        "        trg_len = len(self.trg_data[idx])\n",
        "        return {\"src\": src_sample, \"src_len\": src_len, \"trg\": trg_sample, \"trg_len\": trg_len}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfdJoogSIxAS"
      },
      "source": [
        "def padding_batch(batch):\n",
        "    \"\"\"\n",
        "    input: -> list of dict\n",
        "        [{'src': [1, 2, 3], 'trg': [1, 2, 3]}, {'src': [1, 2, 2, 3], 'trg': [1, 2, 2, 3]}]\n",
        "    output: -> dict of tensor \n",
        "        {\n",
        "            \"src\": [[1, 2, 3, 0], [1, 2, 2, 3]].T\n",
        "            \"trg\": [[1, 2, 3, 0], [1, 2, 2, 3]].T\n",
        "        }\n",
        "    \"\"\"\n",
        "    src_lens = [d[\"src_len\"] for d in batch]\n",
        "    trg_lens = [d[\"trg_len\"] for d in batch]\n",
        "    \n",
        "    src_max = max([d[\"src_len\"] for d in batch])\n",
        "    trg_max = max([d[\"trg_len\"] for d in batch])\n",
        "    for d in batch:\n",
        "        d[\"src\"].extend([en2id[\"<pad>\"]]*(src_max-d[\"src_len\"]))\n",
        "        d[\"trg\"].extend([ch2id[\"<pad>\"]]*(trg_max-d[\"trg_len\"]))\n",
        "    srcs = torch.tensor([pair[\"src\"] for pair in batch], dtype=torch.long, device=device)\n",
        "    trgs = torch.tensor([pair[\"trg\"] for pair in batch], dtype=torch.long, device=device)\n",
        "    \n",
        "    batch = {\"src\":srcs.T, \"src_len\":src_lens, \"trg\":trgs.T, \"trg_len\":trg_lens}\n",
        "    return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN870GeKIzaQ"
      },
      "source": [
        "attention model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1DbYxO9I1nk"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout=0.5, bidirectional=True):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.gru = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        \n",
        "    def forward(self, input_seqs, input_lengths, hidden):\n",
        "        # input_seqs = [seq_len, batch]\n",
        "        embedded = self.embedding(input_seqs)\n",
        "        # embedded = [seq_len, batch, embed_dim]\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, enforce_sorted=False)\n",
        "        \n",
        "        outputs, hidden = self.gru(packed, hidden)        \n",
        "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # outputs = [seq_len, batch, hid_dim * n directions]\n",
        "        # output_lengths = [batch]\n",
        "        return outputs, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QRnFaimI9Hq"
      },
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)  # [seq_len, batch]\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)  # [seq_len, batch, hid_dim]\n",
        "        return torch.sum(hidden * energy, dim=2)  # [seq_len, batch]\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        # hidden.expand(encoder_output.size(0), -1, -1) -> [seq_len, batch, N]\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        # energy = [sql_len, batch, hidden_size]\n",
        "        return torch.sum(self.v * energy, dim=2)  # [seq_len, batch]\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden = [1, batch,  n_directions * hid_dim]\n",
        "        # encoder_outputs = [seq_len, batch, hid dim * n directions]\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        attn_energies = attn_energies.t()  # [batch, seq_len]\n",
        " \n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)  # softmax归一化# [batch, 1, seq_len]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3m4IqN9I_eU"
      },
      "source": [
        "class AttnDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=1, dropout=0.5, bidirectional=True, attn_method=\"general\"):\n",
        "        super(AttnDecoder, self).__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        \n",
        "        if bidirectional:\n",
        "            self.concat = nn.Linear(hid_dim * 2 * 2, hid_dim*2)\n",
        "            self.out = nn.Linear(hid_dim*2, output_dim)\n",
        "            self.attn = Attn(attn_method, hid_dim*2)\n",
        "        else:\n",
        "            self.concat = nn.Linear(hid_dim * 2, hid_dim)\n",
        "            self.out = nn.Linear(hid_dim, output_dim)\n",
        "            self.attn = Attn(attn_method, hid_dim)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, token_inputs, last_hidden, encoder_outputs):\n",
        "        batch_size = token_inputs.size(0)\n",
        "        embedded = self.embedding(token_inputs)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        embedded = embedded.view(1, batch_size, -1) # [1, B, hid_dim]\n",
        "\n",
        "        gru_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # gru_output = [1, batch,  n_directions * hid_dim]\n",
        "        # hidden = [n_layers * n_directions, batch, hid_dim]\n",
        "\n",
        "        # encoder_outputs = [sql_len, batch, hid dim * n directions]\n",
        "        attn_weights = self.attn(gru_output, encoder_outputs)\n",
        "        # attn_weights = [batch, 1, sql_len]\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # [batch, 1, hid_dim * n directions]\n",
        "\n",
        "        # LuongAttention\n",
        "        gru_output = gru_output.squeeze(0) # [batch, n_directions * hid_dim]\n",
        "        context = context.squeeze(1)       # [batch, n_directions * hid_dim]\n",
        "        concat_input = torch.cat((gru_output, context), 1)  # [batch, n_directions * hid_dim * 2]\n",
        "        concat_output = torch.tanh(self.concat(concat_input))  # [batch, n_directions*hid_dim]\n",
        "\n",
        "        output = self.out(concat_output)  # [batch, output_dim]\n",
        "        output = self.softmax(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqUsS9_eJC6-"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 device, \n",
        "                 predict=False, \n",
        "                 basic_dict=None,\n",
        "                 max_len=100\n",
        "                 ):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        \n",
        "        self.device = device\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        self.predict = predict  # 训练阶段还是预测阶段\n",
        "        self.basic_dict = basic_dict  # decoder的字典，存放特殊token对应的id\n",
        "        self.max_len = max_len  # 翻译时最大输出长度\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "        assert encoder.gru.bidirectional == decoder.gru.bidirectional, \\\n",
        "            \"Decoder and encoder must had same value of bidirectional attribute!\"\n",
        "        \n",
        "    def forward(self, input_batches, input_lengths, target_batches=None, target_lengths=None, teacher_forcing_ratio=0.5):\n",
        "        # input_batches = [seq_len, batch]\n",
        "        # target_batches = [seq_len, batch]\n",
        "        batch_size = input_batches.size(1)\n",
        "        \n",
        "        BOS_token = self.basic_dict[\"<bos>\"]\n",
        "        EOS_token = self.basic_dict[\"<eos>\"]\n",
        "        PAD_token = self.basic_dict[\"<pad>\"]\n",
        "\n",
        "        # 初始化\n",
        "        enc_n_layers = self.encoder.gru.num_layers\n",
        "        enc_n_directions = 2 if self.encoder.gru.bidirectional else 1\n",
        "        encoder_hidden = torch.zeros(enc_n_layers*enc_n_directions, batch_size, self.encoder.hid_dim, device=self.device)\n",
        "        \n",
        "        # encoder_outputs = [input_lengths, batch, hid_dim * n directions]\n",
        "        # encoder_hidden = [n_layers*n_directions, batch, hid_dim]\n",
        "        encoder_outputs, encoder_hidden = self.encoder(\n",
        "            input_batches, input_lengths, encoder_hidden)\n",
        "\n",
        "        # 初始化\n",
        "        decoder_input = torch.tensor([BOS_token] * batch_size, dtype=torch.long, device=self.device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        if self.predict:\n",
        "            # 一次只输入一句话\n",
        "            assert batch_size == 1, \"batch_size of predict phase must be 1!\"\n",
        "            output_tokens = []\n",
        "\n",
        "            while True:\n",
        "                decoder_output, decoder_hidden, decoder_attn = self.decoder(\n",
        "                    decoder_input, decoder_hidden, encoder_outputs\n",
        "                )\n",
        "                # [1, 1]\n",
        "                topv, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(1).detach()\n",
        "                output_token = topi.squeeze().detach().item()\n",
        "                if output_token == EOS_token or len(output_tokens) == self.max_len:\n",
        "                    break\n",
        "                output_tokens.append(output_token)\n",
        "            return output_tokens\n",
        "\n",
        "        else:\n",
        "            max_target_length = max(target_lengths)\n",
        "            all_decoder_outputs = torch.zeros((max_target_length, batch_size, self.decoder.output_dim), device=self.device)\n",
        "\n",
        "            for t in range(max_target_length):\n",
        "                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "                if use_teacher_forcing:\n",
        "                    # decoder_output = [batch, output_dim]\n",
        "                    # decoder_hidden = [n_layers*n_directions, batch, hid_dim]\n",
        "                    decoder_output, decoder_hidden, decoder_attn = self.decoder(\n",
        "                        decoder_input, decoder_hidden, encoder_outputs\n",
        "                    )\n",
        "                    all_decoder_outputs[t] = decoder_output\n",
        "                    decoder_input = target_batches[t]  # 下一个输入来自训练数据\n",
        "                else:\n",
        "                    decoder_output, decoder_hidden, decoder_attn = self.decoder(\n",
        "                        decoder_input, decoder_hidden, encoder_outputs\n",
        "                    )\n",
        "                    # [batch, 1]\n",
        "                    topv, topi = decoder_output.topk(1)\n",
        "                    all_decoder_outputs[t] = decoder_output\n",
        "                    decoder_input = topi.squeeze(1).detach()  # 下一个输入来自模型预测\n",
        "            \n",
        "            loss_fn = nn.NLLLoss(ignore_index=PAD_token)\n",
        "            loss = loss_fn(\n",
        "                all_decoder_outputs.reshape(-1, self.decoder.output_dim),  # [batch*seq_len, output_dim]\n",
        "                target_batches.reshape(-1)               # [batch*seq_len]\n",
        "            )\n",
        "            return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7j8j6v1JGoT"
      },
      "source": [
        "train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz5axh_ExxNB"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8-u2zGRxxNB"
      },
      "source": [
        "def train(\n",
        "    model,\n",
        "    data_loader, \n",
        "    optimizer, \n",
        "    clip=1, \n",
        "    teacher_forcing_ratio=0.5, \n",
        "    print_every=None  # None不打印\n",
        "    ):\n",
        "    model.predict = False\n",
        "    model.train()\n",
        "\n",
        "    if print_every == 0:\n",
        "        print_every = 1\n",
        "\n",
        "    print_loss_total = 0  # 每次打印都重置\n",
        "    start = time.time()\n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(data_loader):\n",
        "\n",
        "        # shape = [seq_len, batch]\n",
        "        input_batchs = batch[\"src\"]\n",
        "        target_batchs = batch[\"trg\"]\n",
        "        # list\n",
        "        input_lens = batch[\"src_len\"]\n",
        "        target_lens = batch[\"trg_len\"]\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss = model(input_batchs, input_lens, target_batchs, target_lens, teacher_forcing_ratio)\n",
        "        print_loss_total += loss.item()\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        # 梯度裁剪\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if print_every and (i+1) % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('\\tCurrent Loss: %.4f' % print_loss_avg)\n",
        "\n",
        "    return epoch_loss / len(data_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G25PjRN3xxNC"
      },
      "source": [
        "def evaluate(\n",
        "    model,\n",
        "    data_loader, \n",
        "    print_every=None\n",
        "    ):\n",
        "    model.predict = False\n",
        "    model.eval()\n",
        "    if print_every == 0:\n",
        "        print_every = 1\n",
        "\n",
        "    print_loss_total = 0  # 每次打印都重置\n",
        "    start = time.time()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "\n",
        "            # shape = [seq_len, batch]\n",
        "            input_batchs = batch[\"src\"]\n",
        "            target_batchs = batch[\"trg\"]\n",
        "            # list\n",
        "            input_lens = batch[\"src_len\"]\n",
        "            target_lens = batch[\"trg_len\"]\n",
        "\n",
        "            loss = model(input_batchs, input_lens, target_batchs, target_lens, teacher_forcing_ratio=0)\n",
        "            print_loss_total += loss.item()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if print_every and (i+1) % print_every == 0:\n",
        "                print_loss_avg = print_loss_total / print_every\n",
        "                print_loss_total = 0\n",
        "                print('\\tCurrent Loss: %.4f' % print_loss_avg)\n",
        "\n",
        "    return epoch_loss / len(data_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2OqC1RmxxNC"
      },
      "source": [
        "def translate(\n",
        "    model,\n",
        "    sample, \n",
        "    idx2token=None\n",
        "    ):\n",
        "    model.predict = True\n",
        "    model.eval()\n",
        "\n",
        "    # shape = [seq_len, 1]\n",
        "    input_batch = sample[\"src\"]\n",
        "    # list\n",
        "    input_len = sample[\"src_len\"]\n",
        "\n",
        "    output_tokens = model(input_batch, input_len)\n",
        "    output_tokens = [idx2token[t] for t in output_tokens]\n",
        "\n",
        "    return \"\".join(output_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08UT8H47JF6C"
      },
      "source": [
        "INPUT_DIM = len(en2id)\n",
        "OUTPUT_DIM = len(ch2id)\n",
        "# 超参数\n",
        "BATCH_SIZE = 32\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 1\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "LEARNING_RATE = 1e-4\n",
        "N_EPOCHS = 200\n",
        "CLIP = 1\n",
        "\n",
        "bidirectional = True\n",
        "attn_method = \"general\"\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT, bidirectional)\n",
        "dec = AttnDecoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, bidirectional, attn_method)\n",
        "model = Seq2Seq(enc, dec, device, basic_dict=basic_dict).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le8IiicOxxNC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oYXzqGpJNKA"
      },
      "source": [
        "# 数据集\n",
        "train_set = TranslationDataset(en_num_data, ch_num_data)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=padding_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axHoL9-JJQA1",
        "outputId": "c868a8ea-aec9-44a8-8811-a474da288ce1"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_loader, optimizer, CLIP)\n",
        "    valid_loss = evaluate(model, train_loader)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), \"en2ch-attn-model.pt\")\n",
        "\n",
        "    if epoch %2 == 0:\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 6s\n",
            "\tTrain Loss: 3.886 | Val. Loss: 3.773\n",
            "Epoch: 03 | Time: 0m 6s\n",
            "\tTrain Loss: 3.690 | Val. Loss: 3.490\n",
            "Epoch: 05 | Time: 0m 7s\n",
            "\tTrain Loss: 3.450 | Val. Loss: 3.240\n",
            "Epoch: 07 | Time: 0m 7s\n",
            "\tTrain Loss: 3.254 | Val. Loss: 3.223\n",
            "Epoch: 09 | Time: 0m 7s\n",
            "\tTrain Loss: 3.164 | Val. Loss: 3.278\n",
            "Epoch: 11 | Time: 0m 7s\n",
            "\tTrain Loss: 3.132 | Val. Loss: 3.208\n",
            "Epoch: 13 | Time: 0m 7s\n",
            "\tTrain Loss: 3.078 | Val. Loss: 3.118\n",
            "Epoch: 15 | Time: 0m 8s\n",
            "\tTrain Loss: 3.052 | Val. Loss: 3.098\n",
            "Epoch: 17 | Time: 0m 10s\n",
            "\tTrain Loss: 3.043 | Val. Loss: 3.062\n",
            "Epoch: 19 | Time: 0m 13s\n",
            "\tTrain Loss: 3.012 | Val. Loss: 3.033\n",
            "Epoch: 21 | Time: 0m 14s\n",
            "\tTrain Loss: 2.994 | Val. Loss: 3.024\n",
            "Epoch: 23 | Time: 0m 15s\n",
            "\tTrain Loss: 2.977 | Val. Loss: 3.016\n",
            "Epoch: 25 | Time: 0m 15s\n",
            "\tTrain Loss: 2.963 | Val. Loss: 3.008\n",
            "Epoch: 27 | Time: 0m 18s\n",
            "\tTrain Loss: 2.963 | Val. Loss: 2.983\n",
            "Epoch: 29 | Time: 0m 27s\n",
            "\tTrain Loss: 2.947 | Val. Loss: 2.972\n",
            "Epoch: 31 | Time: 0m 25s\n",
            "\tTrain Loss: 2.928 | Val. Loss: 2.964\n",
            "Epoch: 33 | Time: 0m 26s\n",
            "\tTrain Loss: 2.907 | Val. Loss: 2.959\n",
            "Epoch: 35 | Time: 0m 25s\n",
            "\tTrain Loss: 2.895 | Val. Loss: 2.951\n",
            "Epoch: 37 | Time: 0m 25s\n",
            "\tTrain Loss: 2.892 | Val. Loss: 2.949\n",
            "Epoch: 39 | Time: 0m 25s\n",
            "\tTrain Loss: 2.875 | Val. Loss: 2.954\n",
            "Epoch: 41 | Time: 0m 24s\n",
            "\tTrain Loss: 2.860 | Val. Loss: 2.945\n",
            "Epoch: 43 | Time: 0m 25s\n",
            "\tTrain Loss: 2.834 | Val. Loss: 2.926\n",
            "Epoch: 45 | Time: 0m 25s\n",
            "\tTrain Loss: 2.826 | Val. Loss: 2.937\n",
            "Epoch: 47 | Time: 0m 24s\n",
            "\tTrain Loss: 2.828 | Val. Loss: 2.932\n",
            "Epoch: 49 | Time: 0m 25s\n",
            "\tTrain Loss: 2.817 | Val. Loss: 2.922\n",
            "Epoch: 51 | Time: 0m 21s\n",
            "\tTrain Loss: 2.794 | Val. Loss: 2.918\n",
            "Epoch: 53 | Time: 0m 23s\n",
            "\tTrain Loss: 2.771 | Val. Loss: 2.930\n",
            "Epoch: 55 | Time: 0m 23s\n",
            "\tTrain Loss: 2.754 | Val. Loss: 2.893\n",
            "Epoch: 57 | Time: 0m 23s\n",
            "\tTrain Loss: 2.759 | Val. Loss: 2.924\n",
            "Epoch: 59 | Time: 0m 22s\n",
            "\tTrain Loss: 2.769 | Val. Loss: 2.919\n",
            "Epoch: 61 | Time: 0m 20s\n",
            "\tTrain Loss: 2.741 | Val. Loss: 2.900\n",
            "Epoch: 63 | Time: 0m 21s\n",
            "\tTrain Loss: 2.694 | Val. Loss: 2.910\n",
            "Epoch: 65 | Time: 0m 21s\n",
            "\tTrain Loss: 2.730 | Val. Loss: 2.931\n",
            "Epoch: 67 | Time: 0m 21s\n",
            "\tTrain Loss: 2.700 | Val. Loss: 2.930\n",
            "Epoch: 69 | Time: 0m 21s\n",
            "\tTrain Loss: 2.677 | Val. Loss: 2.917\n",
            "Epoch: 71 | Time: 0m 18s\n",
            "\tTrain Loss: 2.628 | Val. Loss: 2.905\n",
            "Epoch: 73 | Time: 0m 20s\n",
            "\tTrain Loss: 2.637 | Val. Loss: 2.937\n",
            "Epoch: 75 | Time: 0m 18s\n",
            "\tTrain Loss: 2.649 | Val. Loss: 2.922\n",
            "Epoch: 77 | Time: 0m 17s\n",
            "\tTrain Loss: 2.597 | Val. Loss: 2.931\n",
            "Epoch: 79 | Time: 0m 18s\n",
            "\tTrain Loss: 2.649 | Val. Loss: 2.888\n",
            "Epoch: 81 | Time: 0m 17s\n",
            "\tTrain Loss: 2.627 | Val. Loss: 2.920\n",
            "Epoch: 83 | Time: 0m 14s\n",
            "\tTrain Loss: 2.573 | Val. Loss: 2.856\n",
            "Epoch: 85 | Time: 0m 14s\n",
            "\tTrain Loss: 2.558 | Val. Loss: 2.897\n",
            "Epoch: 87 | Time: 0m 13s\n",
            "\tTrain Loss: 2.560 | Val. Loss: 2.851\n",
            "Epoch: 89 | Time: 0m 14s\n",
            "\tTrain Loss: 2.587 | Val. Loss: 2.857\n",
            "Epoch: 91 | Time: 0m 14s\n",
            "\tTrain Loss: 2.542 | Val. Loss: 2.852\n",
            "Epoch: 93 | Time: 0m 14s\n",
            "\tTrain Loss: 2.533 | Val. Loss: 2.870\n",
            "Epoch: 95 | Time: 0m 13s\n",
            "\tTrain Loss: 2.502 | Val. Loss: 2.887\n",
            "Epoch: 97 | Time: 0m 14s\n",
            "\tTrain Loss: 2.469 | Val. Loss: 2.892\n",
            "Epoch: 99 | Time: 0m 14s\n",
            "\tTrain Loss: 2.588 | Val. Loss: 2.851\n",
            "Epoch: 101 | Time: 0m 14s\n",
            "\tTrain Loss: 2.441 | Val. Loss: 2.854\n",
            "Epoch: 103 | Time: 0m 15s\n",
            "\tTrain Loss: 2.355 | Val. Loss: 2.852\n",
            "Epoch: 105 | Time: 0m 15s\n",
            "\tTrain Loss: 2.419 | Val. Loss: 2.841\n",
            "Epoch: 107 | Time: 0m 15s\n",
            "\tTrain Loss: 2.487 | Val. Loss: 2.905\n",
            "Epoch: 109 | Time: 0m 15s\n",
            "\tTrain Loss: 2.356 | Val. Loss: 2.853\n",
            "Epoch: 111 | Time: 0m 17s\n",
            "\tTrain Loss: 2.404 | Val. Loss: 2.888\n",
            "Epoch: 113 | Time: 0m 16s\n",
            "\tTrain Loss: 2.374 | Val. Loss: 2.923\n",
            "Epoch: 115 | Time: 0m 17s\n",
            "\tTrain Loss: 2.350 | Val. Loss: 2.917\n",
            "Epoch: 117 | Time: 0m 16s\n",
            "\tTrain Loss: 2.355 | Val. Loss: 2.894\n",
            "Epoch: 119 | Time: 0m 17s\n",
            "\tTrain Loss: 2.394 | Val. Loss: 2.849\n",
            "Epoch: 121 | Time: 0m 17s\n",
            "\tTrain Loss: 2.327 | Val. Loss: 2.872\n",
            "Epoch: 123 | Time: 0m 17s\n",
            "\tTrain Loss: 2.248 | Val. Loss: 2.987\n",
            "Epoch: 125 | Time: 0m 18s\n",
            "\tTrain Loss: 2.206 | Val. Loss: 2.896\n",
            "Epoch: 127 | Time: 0m 19s\n",
            "\tTrain Loss: 2.422 | Val. Loss: 2.940\n",
            "Epoch: 129 | Time: 0m 19s\n",
            "\tTrain Loss: 2.258 | Val. Loss: 2.990\n",
            "Epoch: 131 | Time: 0m 20s\n",
            "\tTrain Loss: 2.294 | Val. Loss: 2.871\n",
            "Epoch: 133 | Time: 0m 21s\n",
            "\tTrain Loss: 2.293 | Val. Loss: 2.914\n",
            "Epoch: 135 | Time: 0m 20s\n",
            "\tTrain Loss: 2.142 | Val. Loss: 2.952\n",
            "Epoch: 137 | Time: 0m 22s\n",
            "\tTrain Loss: 2.298 | Val. Loss: 2.907\n",
            "Epoch: 139 | Time: 0m 21s\n",
            "\tTrain Loss: 2.143 | Val. Loss: 2.810\n",
            "Epoch: 141 | Time: 0m 22s\n",
            "\tTrain Loss: 2.215 | Val. Loss: 2.907\n",
            "Epoch: 143 | Time: 0m 21s\n",
            "\tTrain Loss: 2.166 | Val. Loss: 2.907\n",
            "Epoch: 145 | Time: 0m 21s\n",
            "\tTrain Loss: 2.109 | Val. Loss: 2.990\n",
            "Epoch: 147 | Time: 0m 21s\n",
            "\tTrain Loss: 2.105 | Val. Loss: 2.741\n",
            "Epoch: 149 | Time: 0m 22s\n",
            "\tTrain Loss: 2.187 | Val. Loss: 2.804\n",
            "Epoch: 151 | Time: 0m 24s\n",
            "\tTrain Loss: 2.172 | Val. Loss: 2.791\n",
            "Epoch: 153 | Time: 0m 24s\n",
            "\tTrain Loss: 2.037 | Val. Loss: 2.882\n",
            "Epoch: 155 | Time: 0m 22s\n",
            "\tTrain Loss: 2.160 | Val. Loss: 2.768\n",
            "Epoch: 157 | Time: 0m 23s\n",
            "\tTrain Loss: 2.050 | Val. Loss: 2.924\n",
            "Epoch: 159 | Time: 0m 24s\n",
            "\tTrain Loss: 2.225 | Val. Loss: 2.850\n",
            "Epoch: 161 | Time: 0m 24s\n",
            "\tTrain Loss: 1.982 | Val. Loss: 2.760\n",
            "Epoch: 163 | Time: 0m 23s\n",
            "\tTrain Loss: 2.168 | Val. Loss: 2.891\n",
            "Epoch: 165 | Time: 0m 23s\n",
            "\tTrain Loss: 2.191 | Val. Loss: 2.916\n",
            "Epoch: 167 | Time: 0m 23s\n",
            "\tTrain Loss: 1.872 | Val. Loss: 2.826\n",
            "Epoch: 169 | Time: 0m 21s\n",
            "\tTrain Loss: 2.153 | Val. Loss: 2.759\n",
            "Epoch: 171 | Time: 0m 22s\n",
            "\tTrain Loss: 2.109 | Val. Loss: 2.698\n",
            "Epoch: 173 | Time: 0m 24s\n",
            "\tTrain Loss: 2.062 | Val. Loss: 2.809\n",
            "Epoch: 175 | Time: 0m 22s\n",
            "\tTrain Loss: 1.941 | Val. Loss: 2.748\n",
            "Epoch: 177 | Time: 0m 23s\n",
            "\tTrain Loss: 1.969 | Val. Loss: 2.661\n",
            "Epoch: 179 | Time: 0m 21s\n",
            "\tTrain Loss: 2.084 | Val. Loss: 2.811\n",
            "Epoch: 181 | Time: 0m 21s\n",
            "\tTrain Loss: 2.083 | Val. Loss: 2.814\n",
            "Epoch: 183 | Time: 0m 19s\n",
            "\tTrain Loss: 1.952 | Val. Loss: 2.823\n",
            "Epoch: 185 | Time: 0m 20s\n",
            "\tTrain Loss: 2.074 | Val. Loss: 2.815\n",
            "Epoch: 187 | Time: 0m 19s\n",
            "\tTrain Loss: 1.845 | Val. Loss: 2.821\n",
            "Epoch: 189 | Time: 0m 20s\n",
            "\tTrain Loss: 1.852 | Val. Loss: 2.860\n",
            "Epoch: 191 | Time: 0m 20s\n",
            "\tTrain Loss: 1.918 | Val. Loss: 2.837\n",
            "Epoch: 193 | Time: 0m 20s\n",
            "\tTrain Loss: 1.785 | Val. Loss: 2.835\n",
            "Epoch: 195 | Time: 0m 19s\n",
            "\tTrain Loss: 1.924 | Val. Loss: 2.695\n",
            "Epoch: 197 | Time: 0m 20s\n",
            "\tTrain Loss: 2.203 | Val. Loss: 2.852\n",
            "Epoch: 199 | Time: 0m 21s\n",
            "\tTrain Loss: 1.873 | Val. Loss: 2.747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avKCefQRJSIi",
        "outputId": "38f90512-b32d-4a35-ac2c-d0444c424bc3"
      },
      "source": [
        "print(\"best valid loss：\", best_valid_loss)\n",
        "# 加载最优权重\n",
        "# model.load_state_dict(torch.load(\"en2ch-attn-model.pt\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best valid loss： 2.661364793777466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP0oD0WuxxND",
        "outputId": "4fa4fb43-e9eb-4d20-d10b-967734e55d48"
      },
      "source": [
        "model.load_state_dict(torch.load(\"en2ch-attn-model.pt\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2QAwvsjJW18"
      },
      "source": [
        "load test data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDWm5DP-xxNE"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq7H-C-AJUW_",
        "outputId": "11a08cba-ac22-44f5-dc27-3fdff61c1bef"
      },
      "source": [
        "random.seed(seed)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "file1=open(\"Result_onelayer.txt\",\"w\",encoding='utf-8')\n",
        "\n",
        "for i in random.sample(range(len(en_num_data)),len(en_num_data)):  \n",
        "    en_tokens = list(filter(lambda x: x!=0, en_num_data[i]))  # 过滤零\n",
        "    ch_tokens = list(filter(lambda x: x!=3 and x!=0, ch_num_data[i]))  # 和机器翻译作对照\n",
        "    sentence = [id2en[t] for t in en_tokens]\n",
        "    print(\"【原文】\")\n",
        "    print(\"\".join(sentence))\n",
        "    translation = [id2ch[t] for t in ch_tokens]\n",
        "    print(\"【原文】\")\n",
        "    print(\"\".join(translation))\n",
        "    test_sample = {}\n",
        "    test_sample[\"src\"] = torch.tensor(en_tokens, dtype=torch.long, device=device).reshape(-1, 1)\n",
        "    test_sample[\"src_len\"] = [len(en_tokens)]\n",
        "    \n",
        "    file1.writelines(translate(model, test_sample, id2ch))\n",
        "    file1.writelines(\"\\n\")\n",
        "    print(\"【机器翻译】\")\n",
        "    print(translate(model, test_sample, id2ch), end=\"\\n\\n\")\n",
        "    \n",
        "file1.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "【原文】\n",
            "Boys dancing on poles in the middle of the night.<eos>\n",
            "【原文】\n",
            "Jungen tanzen mitten in der Nacht auf Pfosten.\n",
            "【机器翻译】\n",
            "Eun  in tan eit an eit an eit an eit an auf Potten.\n",
            "\n",
            "【原文】\n",
            "A little girl climbing into a wooden playhouse.<eos>\n",
            "【原文】\n",
            "Ein kleines Mädchen klettert in ein Spielhaus aus Holz.\n",
            "【机器翻译】\n",
            "Ein kin kleitein keit Sin Siteht aus Heit aus Hooo.\n",
            "\n",
            "【原文】\n",
            "A trendy girl talking on her cellphone while gliding slowly down the street.<eos>\n",
            "【原文】\n",
            "Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.\n",
            "【机器翻译】\n",
            "Ein sicchche  Mäche  in scht  in  it an sicht an sie d an sit an sie d an sie d and seit an sie d an\n",
            "\n",
            "【原文】\n",
            "A man in a blue shirt is standing on a ladder cleaning a window.<eos>\n",
            "【原文】\n",
            "Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.\n",
            "【机器翻译】\n",
            "in  in in ein eit aun eit aun eit aun eit aun eit auf eiter Lut eit aun eit auf eiter Lut eit aun ei\n",
            "\n",
            "【原文】\n",
            "A man is smiling at a stuffed lion<eos>\n",
            "【原文】\n",
            "Ein Mann lächelt einen ausgestopften Löwen an.\n",
            "【机器翻译】\n",
            "in Mann langehtt aun eiten aus eotoo.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XsGrPlcxxNE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}